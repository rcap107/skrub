"""
.. _example_datetime_encoder :

===============================================================
Timeseries forecasting with DatetimeEncoder and lagged features
===============================================================

In this example, we illustrate how combine the datetime features generated by the
|DatetimeEncoder| with the skrub expressions to forecast the the demand for bike
rentals over time.

For the sake of the example, we will use a simple linear model (|RidgeCV|) to
predict the demand in order to highlight the impact of the
datetime features on the prediction performance.

.. |DatetimeEncoder| replace::
    :class:`~skrub.DatetimeEncoder`

.. |TableVectorizer| replace::
    :class:`~skrub.TableVectorizer`

.. |Cleaner| replace::
    :class:`~skrub.Cleaner`

.. |OneHotEncoder| replace::
    :class:`~sklearn.preprocessing.OneHotEncoder`

.. |TimeSeriesSplit| replace::
    :class:`~sklearn.model_selection.TimeSeriesSplit`

.. |ColumnTransformer| replace::
    :class:`~sklearn.compose.ColumnTransformer`

.. |make_column_transformer| replace::
    :class:`~sklearn.compose.make_column_transformer`

.. |RidgeCV| replace::
    :class:`~sklearn.linear_model.RidgeCV`

.. |SimpleImputer| replace::
    :class:`~sklearn.impute.SimpleImputer`

.. |StandardScaler| replace::
    :class:`~sklearn.preprocessing.StandardScaler`

.. |ToDatetime| replace::
    :class:`~skrub.ToDatetime`

.. |var| replace::
    :meth:`skrub.var`

.. |apply_func| replace::
    :meth:`skrub.Expr.skb.apply_func`

.. |choose_bool| replace::
    :meth:`skrub.Expr.skb.choose_bool`

.. |choose_from| replace::
    :meth:`skrub.Expr.skb.choose_from`

.. |train_test_split| replace::
    :meth:`skrub.Expr.skb.train_test_split`

.. |deferred| replace::
    :meth:`skrub.deferred`

.. _scikit-learn example:
 `<https://scikit-learn.org/stable/auto_examples/applications/plot_time_series_lagged_features.html#generating-polars-engineered-lagged-featurehttps://scikit-learn.org/stable/auto_examples/applications/plot_time_series_lagged_features.html#generating-polars-engineered-lagged-featuress>`_

"""

# %%
# A problem with relevant datetime features
# -----------------------------------------
#
# We use a dataset of bike sharing demand in 2011 and 2012.
# In this setting, we want to predict the number of bike rentals, based
# on the date, time and weather conditions.
# In this example, we will use Polars dataframes instead of Pandas.


import polars as pl

from skrub import TableReport, datasets

data = datasets.fetch_bike_sharing().bike_sharing
data = pl.from_pandas(data)
TableReport(data)


# %%
# Prediction with datetime features
# ---------------------------------
#
# In this section, we will use skrub Expressions to build a predictive pipeline
# that pre-processes the data and performs hyperparameter optimization for various
# of the steps involved in the preparation.
# We will use a |RidgeCV| model as our learner. While it is not the most powerful
# model, it will help us illustrate how feature engineering affects the prediction
# performance.


from sklearn.impute import SimpleImputer
from sklearn.linear_model import RidgeCV

###############################################################################
# Since we are working with timeseries, we need to use |TimeSeriesSplit| to perform
# crossvalidation.
from sklearn.model_selection import TimeSeriesSplit
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

ts = TimeSeriesSplit(
    n_splits=3,  # to keep the notebook fast enough on common laptops
    gap=48,  # 2 days (48 hours) data gap between train and test
    max_train_size=10000,  # keep train sets of comparable sizes
    test_size=3000,  # for 2 or 3 digits of precision in scores
)


###############################################################################
# We define a skrub |var| to start with.


import skrub

data_var = skrub.var("data", data)

# We extract our input data (``X``) and the target column (``y``), and mark them
# as X and y.
X = data_var[
    "date", "holiday", "weathersit", "temp", "hum", "windspeed"
].skb.mark_as_X()
y = data_var["cnt"].skb.mark_as_y()
X


# %%
# We can use the |Cleaner| to clean the data and convert the date column to a
# datetime column, as well as performing additional consistency and cleaning checks.
#


from skrub import Cleaner

X = X.skb.apply(Cleaner())

# %%
# Now we define a simple default pipeline for the |RidgeCV| predictor, which includes
# a |StandardScaler| for numerical features and a |SimpleImputer| to handle
# missing values. We will not modify this pipeline: we will instead focus on
# pre-processing and feature engineering.


default_ridge_pipeline = make_pipeline(StandardScaler(), SimpleImputer(), RidgeCV())

# %%
# The "base" variant of the pipeline uses the default |TableVectorizer|, with
# no hyperparameter search. In this case, performing a GridSearch is not
# necessary, but we will do it for consistency with the next steps.

from skrub import TableVectorizer

vectorized = X.skb.apply(TableVectorizer())
predictions_base = vectorized.skb.apply(default_ridge_pipeline, y=y)
search_base = predictions_base.skb.get_grid_search(fitted=True, cv=ts)

# %%
# We can observe the results directly using ``.detailed_results``. Unsurprisingly,
# the results are not very good.


search_base.detailed_results_


# %%
# Prediction with periodic encoders and hyperparameter optimization
# -----------------------------------------------------------------
#
# The |DatetimeEncoder| can generate additional periodic features, which are
# particularly useful for linear models such as Ridge. Periodic features are off
# by default, but they can be enabled by setting the
# ``periodic encoding`` parameter to either  ``circular`` or ``spline``,
# for trigonometric functions or B-Splines respectively.
# Periodic features are added for each part of the date: hour in day, day in week,
# day in month, month in year.
# For our second pipeline, we define a grid of hyperparameters using the skrub
# ``choose_*`` functions. In this case, we use |choose_bool| and |choose_from|,
# as we are not interested in tweaking numerical parameters.
# The ``choose_*`` functions allow to easily prepare a grid of parameters to use
# to perform hyperparameter optimization.
# Here, we use them to turn on and off the ``weekday`` and ``total_seconds`` flag,
# and to select the specific periodic encoder to use.


from sklearn.pipeline import make_pipeline

import skrub.selectors as s
from skrub import DatetimeEncoder

datetime_encoder = DatetimeEncoder(
    add_weekday=skrub.choose_bool(name="weekday"),
    add_total_seconds=skrub.choose_bool(name="total_seconds"),
    periodic_encoding=skrub.choose_from(
        [
            "spline",
            "circular",
            None,
        ],
        name="periodic_encoding",
    ),
)


vectorized = X.skb.apply(datetime_encoder, cols=s.any_date())
predictions_enc = vectorized.skb.apply(default_ridge_pipeline, y=y)

# %%
# Note that, in general, randomized search should be used instead of grid search.
# In this case, grid search is fine as we are interested in a grid of categorical
# values.


search_enc = predictions_enc.skb.get_grid_search(fitted=True, cv=ts)


# %%
# We again observe the results of the grid search using ``.detailed_results_``.

search_enc.detailed_results_

# %%
# We may also use the parallel coordinate plot to visualize the impact of the
# hyperparameters on the prediction performance. It is possible to set a ``min_score``
# to filter out results that have score below the given threshold: in this case,
# we filter out result with a score below ``0.0``.

search_enc.plot_results(min_score=0.0)


# %%
# We can observe that the prediction results have improved a lot thanks to the
# introduction of the periodic features, however they are still not very good.
# We can also see that setting ``total_seconds`` to ``True`` seems to consistently
# reduce the test score.
#
# Adding lagged features
# ----------------------
#
# To further improve the prediction performance, we will extend the
# feature generation step to include lagged features.
# This function is taken from a similar `_scikit-learn example`. As the lagged
# features are based on the real count (i.e., the target feature), we go back to
# the original data to perform feature engineering on that.
#
# To introduce the lagged features, we use a |deferred| function that takes the
# dataframe we are working with, then adds lagged features to it. Given a sample
# in the dataset, lagged features add information relative to samples prior to it.
# In this case, for each hourly sample we add information relative to the previous
# three hours, the same hour on the prior day, as well as aggregate features
# (mean, max, min) obtained by using a rolling mean of either 24 hours, or 7 days.
#
# |deferred| functions delay the execution of the function until it is reached in
# the execution of the skrub expression. They allow arbitrary code to be executed,
# such as operations that leverage the Pandas or Polars API.


# %%
@skrub.deferred
def get_lagged_features(df):
    lagged_df = df.select(
        "cnt",
        *[pl.col("cnt").shift(i).alias(f"lagged_count_{i}h") for i in [1, 2, 3]],
        lagged_count_1d=pl.col("cnt").shift(24),
        lagged_count_1d_1h=pl.col("cnt").shift(24 + 1),
        lagged_count_7d=pl.col("cnt").shift(7 * 24),
        lagged_count_7d_1h=pl.col("cnt").shift(7 * 24 + 1),
        lagged_mean_24h=pl.col("cnt").shift(1).rolling_mean(24),
        lagged_max_24h=pl.col("cnt").shift(1).rolling_max(24),
        lagged_min_24h=pl.col("cnt").shift(1).rolling_min(24),
        lagged_mean_7d=pl.col("cnt").shift(1).rolling_mean(7 * 24),
        lagged_max_7d=pl.col("cnt").shift(1).rolling_max(7 * 24),
        lagged_min_7d=pl.col("cnt").shift(1).rolling_min(7 * 24),
    )
    return lagged_df


# %%
# To add lagged features to the data, we go back to the original ``data_var``, and
# drop the columns that are not relevant. We will have to clean the data again.


data_prep = data_var.drop("casual", "instant", "registered").skb.mark_as_X()
data_prep = data_prep.skb.apply(Cleaner())


# %%
# We can use the |apply_func| method to apply the lagged feature function
# to the data. The lagged features are added to the dataframe, and we can then
# concatenate them with the original data. This is done by using the ``.skb.concat``
# method, which concatenates the two dataframes along either the rows or the columns.
#
# Notice that adding lagged samples will introduce null values for all the initial
# samples, as there is no previous information for them.

data_lagged = data_prep.skb.apply_func(get_lagged_features).drop("cnt")
X = data_prep.skb.concat([data_lagged], axis=1).drop("cnt")
y = data_var["cnt"].skb.mark_as_y()
X


# %%
# We can use the datetime encoder defined above to observe the impact of the new
# features on the predictions.


vectorized = X.skb.apply(TableVectorizer(datetime=datetime_encoder))
predictions_lagged = vectorized.skb.apply(default_ridge_pipeline, y=y)
search_lagged = predictions_lagged.skb.get_grid_search(fitted=True, cv=ts)
search_lagged.detailed_results_


# %%
# We can see that the lagged features improved the prediction performance by a
# large margin. Periodic features are still bringing some benefit.


###############################################################################
# Plotting the prediction
# .......................
#
# To have a better idea of the quality of the predictions, we can plot them directly.
# We can use |train_test_split| to separate the data into a train split and
# a test split.
# In this case, we will use all the data for the year 2011 to predict year 2012.
# The |train_test_split| function can take a custom `splitter` function that returns
# the X and y splits based on some specific requirements; here, we specify a
# cutoff date and use that to prepare the train an test splits.
# For each pipeline, we run a grid search and select only the best pipeline according
# to its test score.
# Then, we train each pipeline on the training split and predict on the test split.


def split_function(X, y, cutoff=None):
    mask = X["date"] < cutoff
    X_train, X_test = X.filter(mask), X.filter(~mask)
    y_train, y_test = y.filter(mask), y.filter(~mask)
    return X_train, X_test, y_train, y_test


###############################################################################
split_base = predictions_base.skb.train_test_split(
    environment=predictions_base.skb.get_data(),
    splitter=split_function,
    cutoff="2012-01-01",
)
search_base = predictions_base.skb.get_grid_search(
    cv=TimeSeriesSplit(), fitted=True
).fit(split_base["train"])
results_base = search_base.best_pipeline_.predict(split_base["test"])

split_enc = predictions_enc.skb.train_test_split(
    environment=predictions_enc.skb.get_data(),
    splitter=split_function,
    cutoff="2012-01-01",
)
search_enc = predictions_enc.skb.get_grid_search(cv=TimeSeriesSplit(), fitted=True).fit(
    split_enc["train"]
)
results_enc = search_enc.best_pipeline_.predict(split_enc["test"])

split_lagged = predictions_lagged.skb.train_test_split(
    environment=predictions_lagged.skb.get_data(),
    splitter=split_function,
    cutoff="2012-01-01",
)
search_lagged = predictions_lagged.skb.get_grid_search(
    cv=TimeSeriesSplit(), fitted=True
).fit(split_lagged["train"])
results_lagged = search_lagged.best_pipeline_.predict(split_lagged["test"])


# %%
# Now we can plot the results. For the sake of the example, we will consider only
# a single week in the year to be able to observe some details.


from datetime import datetime, timedelta

import matplotlib.dates as mdates
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(12, 3), layout="constrained")

X_plot = split_base["X_test"].select(pl.col("date").str.to_datetime())
y_true = split_base["y_test"]
y_base = results_base
y_enc = results_enc
y_lagged = results_lagged

ax.plot(X_plot, y_true, label="Actual demand", linewidth=2, linestyle="--")
ax.plot(X_plot, y_base, label="Default DatetimeEncoder")
ax.plot(X_plot, y_enc, label="Periodic Features")
ax.plot(X_plot, y_lagged, label="Lagged + Periodic Features")

# Consider only the first week of November 2012.
ax.set_xlim([datetime(2012, 11, 1), datetime(2012, 11, 8)])
ax.set_ylim([0, 800])

# Format the x-axis
# Set the x-axis minor ticks to appear every 6 hours and the major ticks to appear
# every day.
ax.xaxis.set_major_formatter(mdates.DateFormatter("%d %h"))
ax.xaxis.set_minor_formatter(mdates.DateFormatter("%Hh"))
ax.xaxis.set_minor_locator(mdates.HourLocator(byhour=range(0, 24, 6)))
ax.xaxis.set_major_locator(mdates.DayLocator())
ax.tick_params(axis="x", which="minor", labelsize=8)
ax.tick_params(axis="x", which="major", pad=10)

ax.set_ylabel("Demand")

# Annotating the days that correspond to the weekend.
annotation_text = "Weekend"

point1 = [datetime(2012, 11, 3, 14, 0, 0), 500]
point2 = [datetime(2012, 11, 4, 14, 0, 0), 550]

td = timedelta(hours=3)

ax.annotate(
    annotation_text,
    xy=point1,
    xytext=(point1[0] + td, point1[1] + 200),
    arrowprops=dict(
        facecolor="black", shrink=0.01, width=0.5, headwidth=5, headlength=5
    ),
    fontsize=10,
)
ax.annotate(
    annotation_text,
    xy=point2,
    xytext=(point1[0] + td, point1[1] + 200),
    arrowprops=dict(
        facecolor="black", shrink=0.01, width=0.5, headwidth=5, headlength=5
    ),
    fontsize=10,
)
fig.legend(loc="center right", bbox_to_anchor=(1.2, 0.5), ncols=1, borderaxespad=0.0)

fig.suptitle(
    "Predicting the demand with linear models and different feature engineering"
    " strategies"
)

# %%
#
# As we can see, the pipeline that uses only the basic RidgeCV model does not
# follow the actual demand: it shows a periodic behavior that matches the days,
# but cannot model properly the peaks.
# The pipeline that includes periodic features is more accurate in modeling the peaks
# from rush hour, however it does not match the different behavior shown in the
# weekends, and it does not match the rush hour peaks either.
# Finally, the pipeline that includes lagged features tracks the actual demand
# very accurately, and is also able to follow the difference in demand on the
# weeekends.
#
#
# Summary
# ^^^^^^^
# In this example we have shown how to combine the Skrub DatetimeEncoder and the
# skrub expressions to perform feature engineering on dates: we generated periodic
# features, and we added lagged features to further extract information from the data.
# We also described how to use skrub expressions to generate hyperparameter grids,
# how to use deferred functions, and how to cross-validate models to produce
# results.
# Finally, we plotted the predictions to observe the effectiveness of the different
# preprocessing strategies.
